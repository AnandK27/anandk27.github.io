<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="IntroStyle: Training-Free Introspective Style Attribution using Diffusion Features">
  <meta name="keywords" content="Image Diffusion, Data Attribution, Style Attribution, Stable Diffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>IntroStyle: Training-Free Introspective Style Attribution using Diffusion Features</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://anandk27.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">IntroStyle: Training-Free Introspective Style Attribution using Diffusion Features</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://anandk27.github.io/">Anand Kumar</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jitengmu.github.io/">Jiteng Mu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="http://www.svcl.ucsd.edu/people/nuno/">Nuno Vasconcelos</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, San Diego</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.14432"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/AnandK27/IntroStyle"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
                </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <img src="./static/images/teaser.png" alt="IntroStyle teaser image." id="teaser" height="100%"/>
      <h2 class="subtitle has-text-centered">
        <span class="texttt">IntroStyle</span> a metric for style measurement.
        Top two rows are retreival results with <span style="color: green;">green</span> colors for correct and <span style="color: rgba(222, 72, 72, 0.84);">red</span> for incorrect retrievals.
        The bottom row shows ranking with lower score for images further away in style from the reference in the first column. 
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h3 class="title has-text-centered is-3">Retrieval Results</h3>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-1">
          <img src="./static/images/car_1.png" id="item-1"
               class="interpolation-image"/>
        </div>
        <div class="item item-1">
          <img src="./static/images/car_2.png" id="item-1"
                class="interpolation-image"/>
        </div>
        <div class="item item-1">
          <img src="./static/images/car_3.png" id="item-1"
               class="interpolation-image"/>
        </div>
        <div class="item item-1">
          <img src="./static/images/car_4.png" id="item-1"
               class="interpolation-image"/>
        </div>
        <div class="item item-1">
          <img src="./static/images/car_5.png" id="item-1"
               class="interpolation-image"/>
        </div>
        
      </div>
      <div class="content has-text-centered">
        The first left column is the query image with remaining columns showing retreival results with <span style="color: green;">green</span> colors for correct and <span style="color: rgba(222, 72, 72, 0.84);">red</span> for incorrect retrievals.
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p> -->
          <p>
            Text-to-image (T2I) models have gained widespread adoption among content creators and the general public. 
            However, this has sparked significant concerns regarding data privacy and copyright infringement among artists. 
            Consequently, there is an increasing demand for T2I models to incorporate mechanisms that prevent the generation of specific artistic styles, thereby safeguarding intellectual property rights.
            Existing methods for style extraction typically necessitate the collection of custom datasets and the training of specialized models.
             This, however, is resource-intensive, time-consuming, and often impractical for real-time applications.
              Moreover, it may not adequately address the dynamic nature of artistic styles and the rapidly evolving landscape of digital art. 
          </p>
          <p>
            We present a novel, training-free framework to solve the style attribution problem, using the features produced by a diffusion model alone, without any external modules or retraining.
             This is denoted as introspective style attribution (<span class="texttt">IntroStyle</span>) and demonstrates superior performance to state-of-the-art models for style retrieval.
              We also introduce a synthetic dataset of Style Hacks (<span class="texttt">SHacks</span>) to isolate artistic style and evaluate fine-grained style attribution performance.
          </p>
          <!-- <p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity.
          </p> -->
        </div>
      </div>
    </div>
    </div>
</section>
    <!--/ Abstract. -->





<section class="section">
  <div class="container is-max-desktop">
    
    <!-- Paper video. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Approach</h2>
        <div class="publication-video">
          <div class="content has-text-justified">
            Our IntroStyle approach leverages a pre-trained diffusion model for extracting style features. We encode the input image into a latent vector using the diffusion model's encoder, noise this latent to a specific timestep t, and pass the noised latent through the denoising network with a null text embedding. We then extract a feature tensor from an intermediate layer of the network, specifically from an up-block. We compute the channel-wise mean μ<sub>c</sub> and variance σ<sub>c</sub><sup>2</sup> for each channel c of this feature tensor. These statistics form our IntroStyle feature representation: f<sup>t,idx</sup>(I) = (μ<sub>1</sub>, ..., μ<sub>C</sub>, σ<sub>1</sub><sup>2</sup>, ..., σ<sub>C</sub><sup>2</sup>)<sup>T</sup>. To compare styles between images, we use the 2-Wasserstein distance between their IntroStyle representations. This simple approach proves remarkably effective for style attribution tasks.
            <br>
          </div>
            <img src="./static/images/architecture.png"
               alt="IntroStyle architecture image." height="100%"/>
          <!-- <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
                  <div class="content has-text-centered">
                  <span class="texttt">IntroStyle</span> computation of style similarity: channel-wise mean µ and variance σ<sup>2</sup> are computed for the identified style layers. Then a distance metric, 2-Wassertein Distance, can be used to measure styles between a pair of images.
                </div>
                </div>
      </div>
    </div>
    <!--/ Paper video. -->

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Style Hacks (<span class="texttt">SHacks</span>) Dataset</h2>
        <div class="content has-text-justified">
          To create our Style Hacks (SHacks) dataset, we curated a collection of prompt-image pairs representing 25 influential artists from the LAION Aesthetics Dataset. We selected each artist's seminal works as reference queries and implemented a systematic prompt-generation strategy. For each original prompt, we derived three variants of two distinct prompt types: "hacked" and "innocent". We used ChatGPT to generate these prompts, employing specific templates to ensure consistency. For "hacked" prompts, we asked ChatGPT to describe the painting without mentioning the artist or artwork name, focusing on distinctive elements and style. For "innocent" prompts, we instructed ChatGPT to simplify the "hacked" prompts by removing style descriptors and keeping only the semantic content. We then used Stable Diffusion v2.1 to synthesize 12 images per prompt, resulting in a diverse reference dataset of 1,800 images. This approach allowed us to isolate stylistic elements effectively, enabling a fine-grained evaluation of style attribution performance.
        </div>
        <img src="./static/images/shacks.png"
             alt="SHacks dataset image." height="100%"/>
          <div class="content has-text-centered">
             Style Hacks (<span class="texttt">SHacks</span>) dataset samples. Each row shows images generated for a reference image by an artist. Reference image, three images generated by “hacked” prompts, and three images generated by “innocent” prompts are shown from left to right.
        
        </div> 
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Comparision with state-of-the-art models</h2>
        <div class="content has-text-justified">
          We compare our retreival method with state-of-the-art models for style attribution. We also use the <span class="texttt">SHacks</span> dataset to evaluate the performance of our method against the baseline models. The results show that our method outperforms the baseline models in fine-grained style attribution tasks.
        </div>
        <img src="./static/images/wikiart_results.png"
        alt="Retreival on Wikiart." height="100%"/>
     <div class="content has-text-centered">
        Wikiart retrieval results.
   </div> 
        <img src="./static/images/shacks_results.png"
             alt="Retreival on SHacks." height="100%"/>
          <div class="content has-text-centered">
             Style Hacks (<span class="texttt">SHacks</span>) retrieval results.
        </div> 
      </div>
    </div>

    <!-- <div class="columns is-centered"> -->

      <!-- Visual Effects. -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div> -->
    <!--/ Matting. -->

    <!-- Animation. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->
<!-- 
      </div>
    </div> -->
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{kumar2024introstyle,
  author    = {Kumar, Anand and Mu, Jiteng and Vasconcelos, Nuno},
  title     = {IntroStyle: Training-Free Introspective Style Attribution using Diffusion Features},
  journal   = {arXiv preprint arXiv: 2412.14432},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <a class="icon-link" href="https://github.com/AnandK27" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Template has been borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
